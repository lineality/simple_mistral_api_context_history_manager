{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Chat with Mistral API\n",
        "This is a colab-notebook for chatting with a Mistral model. This colab can run on any online device that runs a web browser (phone, tablet, laptop, desktop, etc.).\n",
        "\n",
        "This is a system for using the online cloud Mistral api, not the mistral models run locally in a local pipeline.\n",
        "\n",
        "### Note: Colabs are slower\n",
        "Free colabs are amazing for easily sharing and running code in a portable way,\n",
        "but they are slower. Code in production, or run locally, will be faster than a colab.\n",
        "\n",
        "## mistral-small = Mixtral8x7\n",
        "https://mistral.ai/news/mixtral-of-experts/\n",
        "\n",
        "## Steps:\n",
        "- Configure api-key\n",
        "- Select Model: small-8x7 or tiny-7b (optional step)\n",
        "- Select style-prompt to experiment with personality of answer (optional)\n",
        "  - Modify the personality = \"\" text to describe the personality you want.\n",
        "  - Specifying the language of reply can be done in the sytem-prompt\n",
        "- Run all cells [Runtime tab -> run all]\n",
        "- Type text when prompted at bottom of the colab.\n",
        "- Download the saved conversation history if you want. (optional)\n",
        "- Save or download your own copy of the colab under 'File' tab.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ya5gXB9W5e9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### From Mistral docs\n",
        "See:\n",
        "- https://docs.mistral.ai/\n",
        "- https://docs.mistral.ai/platform/client/\n",
        "\n",
        "```\n",
        "curl --location \"https://api.mistral.ai/v1/chat/completions\" \\\n",
        "     --header 'Content-Type: application/json' \\\n",
        "     --header 'Accept: application/json' \\\n",
        "     --header \"Authorization: Bearer $MISTRAL_API_KEY\" \\\n",
        "     --data '{\n",
        "    \"model\": \"mistral-small\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Who is the most renowned French painter?\"}]\n",
        "  }'\n",
        "```"
      ],
      "metadata": {
        "id": "R6fGLCwKndOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Core API call:\n",
        "## response = requests.post(endpoint_url, headers=headers, json=request_body)\n"
      ],
      "metadata": {
        "id": "f_XNy2pxIjg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For Roles and Conversation History\n",
        "\n",
        "\n",
        "```\n",
        "# Set the headers\n",
        "headers = {\n",
        "  \"Content-Type\": \"application/json\",\n",
        "  \"Accept\": \"application/json\",\n",
        "  \"Authorization\": f\"Bearer {mistral_api_key}\"\n",
        "}\n",
        "\n",
        "# Define the request body\n",
        "request_body = {\n",
        "  \"model\": \"mistral-small\",  # 'mistral-small' is 8x7, vs. 'mistral-tiny' for 7b\n",
        "  \"messages\": [{\"role\": \"user\", \"content\": user_input}]\n",
        "}\n",
        "\n",
        "# Send the request\n",
        "response = requests.post(endpoint_url, headers=headers, json=request_body)\n",
        "```\n",
        "or\n",
        "```\n",
        "# Set the headers\n",
        "headers = {\n",
        "  \"Content-Type\": \"application/json\",\n",
        "  \"Accept\": \"application/json\",\n",
        "  \"Authorization\": f\"Bearer {mistral_api_key}\"\n",
        "}\n",
        "\n",
        "\n",
        "conversation_history = [\n",
        "{\"role\": \"system\", \"content\": user_input},\n",
        "{\"role\": \"user\", \"content\": user_input},\n",
        "{\"role\": \"assistant\", \"content\": user_input},\n",
        "{\"role\": \"user\", \"content\": user_input},\n",
        "{\"role\": \"assistant\", \"content\": user_input},\n",
        "{\"role\": \"user\", \"content\": user_input},\n",
        "]\n",
        "\n",
        "# Define the request body\n",
        "request_body = {\n",
        "  \"model\": \"mistral-small\",  # 'mistral-small' is 8x7, vs. 'mistral-tiny' for 7b\n",
        "  \"messages\": conversation_history\n",
        "}\n",
        "\n",
        "# Send the request\n",
        "response = requests.post(endpoint_url, headers=headers, json=request_body)\n",
        "```"
      ],
      "metadata": {
        "id": "a7O-canBpLU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### e.g. From mistral 7b (tiny)\n",
        "\n",
        "(User) Say...\n",
        "- What might a kindly French owner of a bakery say in French to a customer?\n",
        "\n",
        "AI:\n",
        "- A kindly French bakery owner might greet the customer with a warm \"Bonjour, mon ami/madame!\" (Good day, sir/madam!), followed by \"Comment allez-vous aujourd'hui?\" (How are you today?). Once the customer places an order, the bakery owner could respond with a friendly \"C'est délicieux que vous choisissez ce brioche/croissant/pain au chocolat!\" (That's delicious that you choose this croissant/brioche/chocolate bread!), or \"Votre gateau/pain/patisserie est prêt/prête pour vous!\" (Your cake/bread/pastry is ready for you!). Before parting ways, the bakery owner would likely wish the customer a nice day, \"A bien jusqu'à demain!\" (Have a nice day!), or \"A votre santé!\" (To your health!).\n",
        "\n",
        "\n",
        "(User) Say...\n",
        "- Bonjour, je voudrais une baguette, s'il vous plaît.  Avez-vous des croissants ?  Je vais prendre un pain au chocolat aussi.\n",
        "\n",
        "AI:\n",
        "- Bonjour! Bien sûr, je peux t'aider. Une baguette et un pain au chocolat, ça va être un plaisir de te les préparer. Et concernant les croissants, désolé(e) mais nous n'en avons pas pour le moment. À tout à l'heure!\n"
      ],
      "metadata": {
        "id": "ZShRJrnjJ6E6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# login"
      ],
      "metadata": {
        "id": "-GtYYzwxHt2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        ".env: get your environment variables:\n",
        "  Using the Google Secretes (like.env) system\n",
        "  built into colab on the left menu: the 'key' icon.\n",
        "\"\"\"\n",
        "from google.colab import userdata\n",
        "mistral_api_key = userdata.get('mistral_api_key')\n",
        "# mistral_api_key = 'xxx'"
      ],
      "metadata": {
        "id": "BghwWvtCHt-7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Comment out the model you don't want to use."
      ],
      "metadata": {
        "id": "xjmm0B5nL80T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select Model\n",
        "\n",
        "####################\n",
        "# Mistral 7b \"Tiny\"\n",
        "####################\n",
        "use_this_model = \"mistral-tiny\"\n",
        "\n",
        "######################\n",
        "# Mixtral 8x7 \"Small\"\n",
        "######################\n",
        "use_this_model = \"mistral-small\"\n",
        "\n",
        "######################\n",
        "# Mixtral Large ???\n",
        "######################\n",
        "use_this_model = \"mistral-large-latest\""
      ],
      "metadata": {
        "id": "DLHdP9fqMAIi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat W/ History & Saved Files\n",
        "\n",
        "Run all cells."
      ],
      "metadata": {
        "id": "j2nfxcae5plc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Npzlz9WGm549"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# mistral_api_key = userdata.get('mistral_api_key')\n",
        "\n",
        "# Define the endpoint URL\n",
        "endpoint_url = \"https://api.mistral.ai/v1/chat/completions\"\n",
        "\n",
        "# Set the headers\n",
        "headers = {\n",
        "  \"Content-Type\": \"application/json\",\n",
        "  \"Accept\": \"application/json\",\n",
        "  \"Authorization\": f\"Bearer {mistral_api_key}\"\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "# mode: [{\"role\": \"user\", \"content\": \"say yes\"}]\n",
        "\n",
        "    # Define the request body\n",
        "    request_body = {\n",
        "      \"model\": \"mistral-small\",\n",
        "      \"messages\": [{\"role\": \"user\", \"content\": user_input}]\n",
        "    }\n",
        "\n",
        "    # Send the request\n",
        "    response = requests.post(endpoint_url, headers=headers, json=request_body)\n",
        "\"\"\"\n",
        "\n",
        "# conversation-history setup\n",
        "context_history = []\n",
        "\n",
        "\n",
        "def add_to_context_history(role, comment):\n",
        "\n",
        "    if role == 'user':\n",
        "        segment = {\"role\": \"user\", \"content\": comment}\n",
        "\n",
        "    elif role == 'assistant':\n",
        "        segment = {\"role\": \"assistant\", \"content\": comment}\n",
        "\n",
        "    elif role == 'system':\n",
        "        segment = {\"role\": \"system\", \"content\": comment}\n",
        "\n",
        "    else:\n",
        "        print(\"add_to_context_history(role, comment)\")\n",
        "        print(role, comment)\n",
        "        print('error')\n",
        "\n",
        "    return segment\n",
        "\n",
        "\n",
        "def prompt_user(user_input, context_history):\n",
        "\n",
        "    context_history.append( add_to_context_history(\"user\", user_input) )\n",
        "\n",
        "    return context_history\n",
        "\n",
        "\n",
        "def ask_mistral_tiny(context_history):\n",
        "    # Define the request body\n",
        "    request_body = {\n",
        "      \"model\": use_this_model,\n",
        "      \"messages\": context_history\n",
        "    }\n",
        "\n",
        "    # Send the request\n",
        "    response = requests.post(endpoint_url, headers=headers, json=request_body)\n",
        "\n",
        "    # Check the response status code\n",
        "    if response.status_code != 200:\n",
        "      raise Exception(f\"Error: {response.status_code} {response.text}\")\n",
        "\n",
        "    return response\n",
        "\n",
        "def print_rec_ai(response, context_history):\n",
        "\n",
        "    # Get the response data\n",
        "    response_data = response.json()\n",
        "\n",
        "    # Print the Mistral response\n",
        "\n",
        "    ##\n",
        "    ##\n",
        "    # Turn this print on to see full return data\n",
        "    ##\n",
        "    ##\n",
        "    \"\"\"\n",
        "    e.g.\n",
        "    {\n",
        "      \"id\": \"635cb8d445ujhe5546bb64e5e7\",\n",
        "      \"object\": \"chat.completion\",\n",
        "      \"created\": 170hrjfjf7084,\n",
        "      \"model\": \"mistral-tiny\",\n",
        "      \"choices\": [\n",
        "        {\n",
        "          \"index\": 0,\n",
        "          \"message\": {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"Enjoy your cup of tea!\"\n",
        "          },\n",
        "          \"finish_reason\": \"stop\",\n",
        "          \"logprobs\": null\n",
        "        }\n",
        "      ],\n",
        "      \"usage\": {\n",
        "        \"prompt_tokens\": 575,\n",
        "        \"total_tokens\": 629,\n",
        "        \"completion_tokens\": 54\n",
        "      }\n",
        "    }\n",
        "    \"\"\"\n",
        "    # print(json.dumps(response_data, indent=2))\n",
        "    # print(type(response_data))\n",
        "\n",
        "    output = response_data\n",
        "    # print(type(output))\n",
        "    # print(type(output[\"choices\"][0]))\n",
        "\n",
        "    # extract just the 'what they said' part out\n",
        "    assistant_says = output[\"choices\"][0]['message']['content']\n",
        "\n",
        "    # print(assistant_says)\n",
        "\n",
        "    new_comment = {\"role\": \"assistant\", \"content\": assistant_says}\n",
        "\n",
        "    # add what assistant said to context history\n",
        "    context_history.append(new_comment)\n",
        "\n",
        "    return assistant_says, context_history\n",
        "\n",
        "\n",
        "def go_user(user_input, context_history):\n",
        "    \"\"\"\n",
        "    Input: context_history\n",
        "    Ouput Tuple: assistant_says, context_history\n",
        "    \"\"\"\n",
        "\n",
        "    # prompt user\n",
        "    context_history = prompt_user(user_input, context_history)\n",
        "\n",
        "    # prompt assistant\n",
        "    response = ask_mistral_tiny(context_history)\n",
        "\n",
        "    # ETL: Extract, Transform, & Load\n",
        "    assistant_says, context_history = print_rec_ai(response, context_history)\n",
        "\n",
        "    return assistant_says, context_history\n",
        "\n",
        "\n",
        "def strip_non_alpha(text):\n",
        "    # regex to leave only a-z characters\n",
        "    pattern = re.compile('[^a-z]')\n",
        "    return pattern.sub('', text).lower()\n",
        "\n",
        "\n",
        "def keep_talking(context_history):\n",
        "    \"\"\"\n",
        "    A very minimal chat with memory.\n",
        "\n",
        "    Uses:\n",
        "      query(input_string)\n",
        "      strip_non_alpha(text)\n",
        "    \"\"\"\n",
        "    still_talking = True\n",
        "    dialogue_history = \"\"\n",
        "\n",
        "    while still_talking:\n",
        "\n",
        "        user_input = input(\"Say...\")\n",
        "\n",
        "        exit_phrase_list = [\n",
        "            \"exit\",\n",
        "            \"quit\",\n",
        "            \"quite\",\n",
        "            \"!q\",\n",
        "            \"q\",\n",
        "            \"done\",\n",
        "            \"finish\",\n",
        "            \"end\",\n",
        "            \"bye\",\n",
        "            \"good bye\",\n",
        "        ]\n",
        "\n",
        "        # check if user is exiting convesation\n",
        "        if strip_non_alpha(user_input) in exit_phrase_list:\n",
        "            print(\"\\nAll Done!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            assistant_says, context_history = go_user(user_input, context_history)\n",
        "\n",
        "            print( assistant_says )\n",
        "\n",
        "            # save dialogue so far\n",
        "            dialogue_history = context_history\n",
        "\n",
        "    # when out of loop, return history\n",
        "    return dialogue_history\n",
        "\n",
        "\n",
        "# save history\n",
        "def record_history_save_files(dialogue_history):\n",
        "\n",
        "    date_time = dt.utcnow()\n",
        "    timestamp = date_time.strftime('%Y/%m/%d  %H:%M:%S:%f')\n",
        "    clean_timestamp = date_time.strftime('%Y%m%d%H%M')\n",
        "\n",
        "    # To save the data directly as a JSON file:\n",
        "\n",
        "    # Convert the Python dictionary list to a JSON string\n",
        "    json_data = json.dumps(dialogue_history)\n",
        "\n",
        "    # Open a file for writing in JSON format\n",
        "    with open(f'json_dialog_{clean_timestamp}.json', 'w') as json_file:\n",
        "        # Write the JSON string to the file\n",
        "        json_file.write(json_data)\n",
        "\n",
        "\n",
        "    # To save the data as a file readable as a script:\n",
        "\n",
        "    # Create a new file for writing\n",
        "    with open(f'script_dialog_{clean_timestamp}.txt', 'w') as script_file:\n",
        "\n",
        "        # add timestamp\n",
        "        text = timestamp + \"\\n\\n\"\n",
        "        script_file.write(text)\n",
        "\n",
        "        # Iterate over the dictionary list\n",
        "        for item in dialogue_history:\n",
        "            # Write the role and content of each item to the file, separated by a newline\n",
        "            script_file.write(f\"{item['role']}: {item['content']}\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set 'Personality Instruction' and Reset History\n",
        "\n",
        "Tech note: This sets a 'system prompt'"
      ],
      "metadata": {
        "id": "t7je4Jg6nU0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset History\n",
        "context_history = []\n",
        "\n",
        "######################################\n",
        "# Enter your Personality request here\n",
        "######################################\n",
        "\n",
        "# e.g. A Parent\n",
        "personality = \"\"\"You are a helpful chatbot who always answers with the\n",
        "personality or persona and in the style\n",
        "in which this excerpt was written:\n",
        "\n",
        "My dearest Ada,\n",
        "\n",
        "I hope camp is treating you well!  Are you making new friends and exploring all the fun activities? Remember that time we built the world's biggest blanket fort in the living room? You have that same spirit of adventure in you.  Try new things, even if they seem a bit scary at first. Don't be afraid to ask your counselors for help, they're there to make your time awesome.\n",
        "\n",
        "I miss our bedtime stories and silly songs, but I'm so excited for all the stories you'll bring home!  I love you more than all the stars in the campfire sky.\n",
        "\n",
        "Love always,\n",
        "Mom\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# e.g. A Duck\n",
        "personality = \"\"\"You are a helpful chatbot who always answers in the style\n",
        "of a duck who quacks.\n",
        "\"\"\"\n",
        "\n",
        "# e.g. Generic Warm Person\n",
        "personality = \"\"\"You are a helpful chatbot who always answers in the style\n",
        "of a warm, friendly, outgoing friend, who uses only the language\n",
        "of the user. If someone speaks to you not in English, use only\n",
        "their language to reply. No translation into English, only speak and respond\n",
        "entirely in the language used by User to speak with you.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Save the lower-most 'personality' description from above.\n",
        "context_history.append(add_to_context_history('system', personality))"
      ],
      "metadata": {
        "id": "uloouSMrvhRn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime as dt\n",
        "\n",
        "# make readable time\n",
        "date_time = dt.utcnow()\n",
        "timestamp = date_time.strftime('%Y/%m/%d  %H:%M:%S:%f')\n",
        "clean_timestamp = date_time.strftime('%Y%m%d%H%M%S%f')\n",
        "\n",
        "instructions = f\"\"\"\n",
        "  Instructions: {timestamp}\n",
        "    - Enter your queries into the iput-box.\n",
        "    - Say 'quit' or 'exit', etc.,  to leave the conversation.\n",
        "\n",
        "\"\"\"\n",
        "print( instructions )\n",
        "\n",
        "print( context_history )\n",
        "\n",
        "# run chat\n",
        "dialogue_history = keep_talking(context_history)\n",
        "\n",
        "# save history files\n",
        "record_history_save_files(dialogue_history)"
      ],
      "metadata": {
        "id": "kcab3MZ-wFIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef69b1b1-fb30-4080-8a4b-7ab790c21509"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Instructions: 2024/02/28  13:47:42:313258\n",
            "    - Enter your queries into the iput-box.\n",
            "    - Say 'quit' or 'exit', etc.,  to leave the conversation.\n",
            "\n",
            "\n",
            "[{'role': 'system', 'content': 'You are a helpful chatbot who always answers in the style\\nof a warm, friendly, outgoing friend, who uses only the language\\nof the user. If someone speaks to you not in English, use only\\ntheir language to reply. No translation into English, only speak and respond\\nentirely in the language used by User to speak with you.\\n'}]\n",
            "Say...Would Alan tuing have joined the summer and AI had he lived?\n",
            "It is impossible to know for certain what Alan Turing would have done had he lived, as he passed away in 1954 and the field of artificial intelligence has developed significantly since then. However, Turing was a pioneer in the field of computer science and was interested in the possibility of creating intelligent machines, so it is possible that he would have been involved in the development of AI in some capacity. Whether he would have specifically joined a project like the \"summer and AI\" would depend on the specific circumstances and his interests at the time.\n",
            "Say...exit\n",
            "\n",
            "All Done!\n"
          ]
        }
      ]
    }
  ]
}