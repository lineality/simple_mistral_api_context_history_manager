{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOWHGobqXExFNvDV4OchTwu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Simple Chat with Mistral API\n","This is a colab-notebook for chatting with a Mistral model. This colab can run on any online device that runs a web browser (phone, tablet, laptop, desktop, etc.).\n","\n","This is a system for using the online cloud Mistral api, not the mistral models run locally in a local pipeline.\n","\n","### Note: Colabs are slower\n","Free colabs are amazing for easily sharing and running code in a portable way,\n","but they are slower. Code in production, or run locally, will be faster than a colab.\n","\n","## mistral-small-latest = Mixtral8x7\n","https://mistral.ai/news/mixtral-of-experts/\n","\n","## Steps:\n","- Configure api-key\n","- Select Model: small-8x7 or open-mistral-7b (optional step)\n","- Select style-prompt to experiment with personality of answer (optional)\n","  - Modify the personality = \"\" text to describe the personality you want.\n","  - Specifying the language of reply can be done in the sytem-prompt\n","- Run all cells [Runtime tab -> run all]\n","- Type text when prompted at bottom of the colab.\n","- Download the saved conversation history if you want. (optional)\n","- Save or download your own copy of the colab under 'File' tab.\n","\n","\n","\n","\n"],"metadata":{"id":"Ya5gXB9W5e9G"}},{"cell_type":"markdown","source":["#### From Mistral docs\n","See:\n","- https://docs.mistral.ai/\n","- https://docs.mistral.ai/platform/client/\n","\n","```\n","curl --location \"https://api.mistral.ai/v1/chat/completions\" \\\n","     --header 'Content-Type: application/json' \\\n","     --header 'Accept: application/json' \\\n","     --header \"Authorization: Bearer $MISTRAL_API_KEY\" \\\n","     --data '{\n","    \"model\": \"mistral-small-latest\",\n","    \"messages\": [{\"role\": \"user\", \"content\": \"Who is the most renowned French painter?\"}]\n","  }'\n","```"],"metadata":{"id":"R6fGLCwKndOE"}},{"cell_type":"markdown","source":["# The Core API call:\n","## response = requests.post(endpoint_url, headers=headers, json=request_body)\n"],"metadata":{"id":"f_XNy2pxIjg-"}},{"cell_type":"markdown","source":["# For Roles and Conversation History\n","\n","\n","## For Simple No-history Mode:\n","```\n","# Set the headers\n","headers = {\n","  \"Content-Type\": \"application/json\",\n","  \"Accept\": \"application/json\",\n","  \"Authorization\": f\"Bearer {mistral_api_key}\"\n","}\n","\n","# Define the request body\n","request_body = {\n","  \"model\": \"mistral-small-latest\",  # 'mistral-small' is 8x7, vs. 'open-mistral-7b' for 7b\n","  \"messages\": [{\"role\": \"user\", \"content\": user_input}]\n","}\n","\n","# Send the request\n","response = requests.post(endpoint_url, headers=headers, json=request_body)\n","\n","# extract just the 'what they said' part out\n","\n","    # Get the response data\n","    response_data = response.json()\n","\n","    # Print the Mistral response\n","\n","    ##\n","    ##\n","    # Turn this print on to see full return data\n","    ##\n","    ##\n","    \"\"\"\n","    e.g.\n","    {\n","      \"id\": \"635cb8d445ujhe5546bb64e5e7\",\n","      \"object\": \"chat.completion\",\n","      \"created\": 170hrjfjf7084,\n","      \"model\": \"open-mistral-7b\",\n","      \"choices\": [\n","        {\n","          \"index\": 0,\n","          \"message\": {\n","            \"role\": \"assistant\",\n","            \"content\": \"Enjoy your cup of tea!\"\n","          },\n","          \"finish_reason\": \"stop\",\n","          \"logprobs\": null\n","        }\n","      ],\n","      \"usage\": {\n","        \"prompt_tokens\": 575,\n","        \"total_tokens\": 629,\n","        \"completion_tokens\": 54\n","      }\n","    }\n","    \"\"\"\n","    # print(json.dumps(response_data, indent=2))\n","    # print(type(response_data))\n","\n","    output = response_data\n","    # print(type(output))\n","    # print(type(output[\"choices\"][0]))\n","\n","    # extract just the 'what they said' part out\n","    assistant_says = output[\"choices\"][0]['message']['content']\n","```\n","or\n","## For History Mode:\n","```\n","# Set the headers\n","headers = {\n","  \"Content-Type\": \"application/json\",\n","  \"Accept\": \"application/json\",\n","  \"Authorization\": f\"Bearer {mistral_api_key}\"\n","}\n","\n","\n","conversation_history = [\n","{\"role\": \"system\", \"content\": user_input},\n","{\"role\": \"user\", \"content\": user_input},\n","{\"role\": \"assistant\", \"content\": user_input},\n","{\"role\": \"user\", \"content\": user_input},\n","{\"role\": \"assistant\", \"content\": user_input},\n","{\"role\": \"user\", \"content\": user_input},\n","]\n","\n","# Define the request body\n","request_body = {\n","  \"model\": \"mistral-small-latest\",  # 'mistral-small' is 8x7, vs. 'open-mistral-7b' for 7b\n","  \"messages\": conversation_history\n","}\n","\n","# Send the request\n","response = requests.post(endpoint_url, headers=headers, json=request_body)\n","\n","# extract just the 'what they said' part out\n","\n","    # Get the response data\n","    response_data = response.json()\n","\n","    # Print the Mistral response\n","\n","    ##\n","    ##\n","    # Turn this print on to see full return data\n","    ##\n","    ##\n","    \"\"\"\n","    e.g.\n","    {\n","      \"id\": \"635cb8d445ujhe5546bb64e5e7\",\n","      \"object\": \"chat.completion\",\n","      \"created\": 170hrjfjf7084,\n","      \"model\": \"open-mistral-7b\",\n","      \"choices\": [\n","        {\n","          \"index\": 0,\n","          \"message\": {\n","            \"role\": \"assistant\",\n","            \"content\": \"Enjoy your cup of tea!\"\n","          },\n","          \"finish_reason\": \"stop\",\n","          \"logprobs\": null\n","        }\n","      ],\n","      \"usage\": {\n","        \"prompt_tokens\": 575,\n","        \"total_tokens\": 629,\n","        \"completion_tokens\": 54\n","      }\n","    }\n","    \"\"\"\n","    # print(json.dumps(response_data, indent=2))\n","    # print(type(response_data))\n","\n","    output = response_data\n","    # print(type(output))\n","    # print(type(output[\"choices\"][0]))\n","\n","    # extract just the 'what they said' part out\n","    assistant_says = output[\"choices\"][0]['message']['content']\n","```"],"metadata":{"id":"a7O-canBpLU9"}},{"cell_type":"markdown","source":["### e.g. From mistral 7b (tiny)\n","\n","(User) Say...\n","- What might a kindly French owner of a bakery say in French to a customer?\n","\n","AI:\n","- A kindly French bakery owner might greet the customer with a warm \"Bonjour, mon ami/madame!\" (Good day, sir/madam!), followed by \"Comment allez-vous aujourd'hui?\" (How are you today?). Once the customer places an order, the bakery owner could respond with a friendly \"C'est délicieux que vous choisissez ce brioche/croissant/pain au chocolat!\" (That's delicious that you choose this croissant/brioche/chocolate bread!), or \"Votre gateau/pain/patisserie est prêt/prête pour vous!\" (Your cake/bread/pastry is ready for you!). Before parting ways, the bakery owner would likely wish the customer a nice day, \"A bien jusqu'à demain!\" (Have a nice day!), or \"A votre santé!\" (To your health!).\n","\n","\n","(User) Say...\n","- Bonjour, je voudrais une baguette, s'il vous plaît.  Avez-vous des croissants ?  Je vais prendre un pain au chocolat aussi.\n","\n","AI:\n","- Bonjour! Bien sûr, je peux t'aider. Une baguette et un pain au chocolat, ça va être un plaisir de te les préparer. Et concernant les croissants, désolé(e) mais nous n'en avons pas pour le moment. À tout à l'heure!\n"],"metadata":{"id":"ZShRJrnjJ6E6"}},{"cell_type":"markdown","source":["# login"],"metadata":{"id":"-GtYYzwxHt2s"}},{"cell_type":"code","source":["\"\"\"\n",".env: get your environment variables:\n","  Using the Google Secretes (like.env) system\n","  built into colab on the left menu: the 'key' icon.\n","\"\"\"\n","from google.colab import userdata\n","mistral_api_key = userdata.get('mistral_api_key')\n","\n","\n","\"\"\"\n","Python Dot-env\n","\"\"\"\n","# from dotenv import load_dotenv\n","# import os\n","\n","# load_dotenv()\n","# api_key = os.getenv(\"mistral_api_key\")\n","\n","\n","\"\"\"\n","Hard Code (not the best idea)\n","\"\"\"\n","# mistral_api_key = 'xxx'"],"metadata":{"id":"BghwWvtCHt-7","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1720459230417,"user_tz":240,"elapsed":775,"user":{"displayName":"On Off","userId":"17227537208113104618"}},"outputId":"eb5cf5ee-bbb5-4c6a-9633-152734733fd4"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nHard Code (not the best idea)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# Setup\n","\n","Comment out the model you don't want to use."],"metadata":{"id":"xjmm0B5nL80T"}},{"cell_type":"code","source":["# Select Model\n","\"\"\"\n","https://docs.mistral.ai/api/\n","\n","open-mistral-7b\n","\n","open-mixtral-8x22b\n","open-mixtral-8x22b-2404\n","\n","codestral-latest\n","codestral-2405\n","\n","\n","open-mistral-7b\n","(aka mistral-tiny-2312)\n","renamed from mistral-tiny\n","The endpoint mistral-tiny will be deprecated\n","\n","\n","Feb. 26, 2024\n","\n","API endpoints: We renamed 3 API endpoints and added 2 model endpoints.\n","\n","open-mistral-7b (aka mistral-tiny-2312): renamed from mistral-tiny. The endpoint mistral-tiny will be deprecated in three months.\n","open-mixtral-8x7B (aka mistral-small-2312): renamed from mistral-small. The endpoint mistral-small will be deprecated in three months.\n","mistral-small-latest (aka mistral-small-2402): new model.\n","mistral-medium-latest (aka mistral-medium-2312): old model. The previous mistral-medium has been dated and tagged as mistral-medium-2312. The endpoint mistral-medium will be deprecated in three months.\n","mistral-large-latest (aka mistral-large-2402): our new flagship model with leading performance.\n","\n","\"\"\"\n","\n","##################\n","# Open Mistral 7b\n","##################\n","# previously \"tiny\"\n","use_this_model = \"open-mistral-7b\"\n","\n","\n","###################\n","# Open Mixtral 8x7\n","###################\n","# previously \"small\"\n","use_this_model = \"open-mixtral-8x7B\"\n","\n","\n","######################\n","# open mixtral 8x22b\n","######################\n","# ...was 'medium'?\n","use_this_model = \"open-mixtral-8x22b\"\n","\n","\n","#######################\n","# Small, Medium, Large  (no 'tiny')\n","#######################\n","use_this_model = \"mistral-small-latest\"\n","use_this_model = \"mistral-medium-latest\"\n","use_this_model = \"mistral-large-latest\"\n","\n","##############\n","# Codestral\n","##############\n","use_this_model = \"codestral-latest\""],"metadata":{"id":"DLHdP9fqMAIi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["use_this_model = \"open-mistral-7b\""],"metadata":{"id":"A0OgjAcEXrtj","executionInfo":{"status":"ok","timestamp":1720459149486,"user_tz":240,"elapsed":3,"user":{"displayName":"On Off","userId":"17227537208113104618"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Chat W/ History & Saved Files\n","\n","Run all cells."],"metadata":{"id":"j2nfxcae5plc"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"Npzlz9WGm549","executionInfo":{"status":"ok","timestamp":1720459150173,"user_tz":240,"elapsed":239,"user":{"displayName":"On Off","userId":"17227537208113104618"}}},"outputs":[],"source":["import requests\n","import json\n","import os\n","import re\n","\n","from google.colab import userdata\n","\n","\"\"\"\n","# mistral_api_key = userdata.get('mistral_api_key')\n","\n","# Define the endpoint URL\n","endpoint_url = \"https://api.mistral.ai/v1/chat/completions\"\n","\n","# Set the headers\n","headers = {\n","  \"Content-Type\": \"application/json\",\n","  \"Accept\": \"application/json\",\n","  \"Authorization\": f\"Bearer {mistral_api_key}\"\n","}\n","\n","# mode: [{\"role\": \"user\", \"content\": \"say yes\"}]\n","\n","    # Define the request body\n","    request_body = {\n","      \"model\": \"mistral-small-latest\",\n","      \"messages\": [{\"role\": \"user\", \"content\": user_input}]\n","    }\n","\n","    # Send the request\n","    response = requests.post(endpoint_url, headers=headers, json=request_body)\n","\"\"\"\n","\n","# conversation-history setup\n","context_history = []\n","\n","\n","def print_rec_ai(response, context_history):\n","\n","    # Get the response data\n","    response_data = response.json()\n","\n","    # Print the Mistral response\n","\n","    ##\n","    ##\n","    # Turn this print on to see full return data\n","    ##\n","    ##\n","    \"\"\"\n","    e.g.\n","    {\n","      \"id\": \"635cb8d445ujhe5546bb64e5e7\",\n","      \"object\": \"chat.completion\",\n","      \"created\": 170hrjfjf7084,\n","      \"model\": \"open-mistral-7b\",\n","      \"choices\": [\n","        {\n","          \"index\": 0,\n","          \"message\": {\n","            \"role\": \"assistant\",\n","            \"content\": \"Enjoy your cup of tea!\"\n","          },\n","          \"finish_reason\": \"stop\",\n","          \"logprobs\": null\n","        }\n","      ],\n","      \"usage\": {\n","        \"prompt_tokens\": 575,\n","        \"total_tokens\": 629,\n","        \"completion_tokens\": 54\n","      }\n","    }\n","    \"\"\"\n","    # print(json.dumps(response_data, indent=2))\n","    # print(type(response_data))\n","\n","    output = response_data\n","    # print(type(output))\n","    # print(type(output[\"choices\"][0]))\n","\n","    # extract just the 'what they said' part out\n","    assistant_says = output[\"choices\"][0]['message']['content']\n","\n","    # print(assistant_says)\n","\n","    new_comment = {\"role\": \"assistant\", \"content\": assistant_says}\n","\n","    # add what assistant said to context history\n","    context_history.append(new_comment)\n","\n","    return assistant_says, context_history\n","\n","def add_to_context_history(role, comment):\n","\n","    if role == 'user':\n","        segment = {\"role\": \"user\", \"content\": comment}\n","\n","    elif role == 'assistant':\n","        segment = {\"role\": \"assistant\", \"content\": comment}\n","\n","    elif role == 'system':\n","        segment = {\"role\": \"system\", \"content\": comment}\n","\n","    else:\n","        print(\"add_to_context_history(role, comment)\")\n","        print(role, comment)\n","        print('error')\n","\n","    return segment\n","\n","\n","def prompt_user(user_input, context_history):\n","\n","    context_history.append( add_to_context_history(\"user\", user_input) )\n","\n","    return context_history\n","\n","\n","def go_user(user_input, context_history, use_this_model):\n","    \"\"\"\n","    Input: context_history\n","    Ouput Tuple: assistant_says, context_history\n","    \"\"\"\n","\n","    # prompt user\n","    context_history = prompt_user(user_input, context_history)\n","\n","    # prompt assistant\n","    response = ask_mistral_api(context_history, use_this_model)\n","\n","    # ETL: Extract, Transform, & Load\n","    assistant_says, context_history = print_rec_ai(response, context_history)\n","\n","    return assistant_says, context_history\n","\n","\n","def ask_mistral_api(context_history, use_this_model):\n","\n","\n","    # Define the endpoint URL\n","    endpoint_url = \"https://api.mistral.ai/v1/chat/completions\"\n","\n","    # Set the headers\n","    headers = {\n","      \"Content-Type\": \"application/json\",\n","      \"Accept\": \"application/json\",\n","      \"Authorization\": f\"Bearer {mistral_api_key}\"\n","    }\n","\n","    # Define the request body\n","    request_body = {\n","      \"model\": use_this_model,\n","      \"messages\": context_history\n","    }\n","\n","    #################\n","    #################\n","    # Hit the ai api\n","    #################\n","    #################\n","    # Send the request\n","    response = requests.post(endpoint_url, headers=headers, json=request_body)\n","\n","    # Check the response status code\n","    if response.status_code != 200:\n","      raise Exception(f\"Error: {response.status_code} {response.text}\")\n","\n","    return response\n","\n","\n","def simple_ask_mistral_cloud(input_string, use_this_model):\n","    \"\"\"\n","    you have: a string\n","    you need: a response\n","\n","    1. make minimal history contexxt\n","    2. make a generic system instruction, for show\n","    3. make system-user context: string input\n","    4. ask mistral for that model\n","    5. extract just the response string\n","    6. return only reply (no 'history')\n","    \"\"\"\n","\n","    # 1. make minimal history contexxt\n","    context_history = []\n","\n","    # 2. make a generic system instruction\n","    generic_system_instruction = \"You are helpful and answer accurately.\"\n","    context_history.append( add_to_context_history(\"system\", generic_system_instruction) )\n","\n","    # 3. make system-user context: string input\n","    context_history.append( add_to_context_history(\"user\", input_string) )\n","\n","    # 4. ask mistral for that model\n","    response = ask_mistral_api(context_history, use_this_model)\n","\n","\n","    # Get the response data\n","    response_data = response.json()\n","\n","\n","    # 5. extract just the response string\n","\n","    ##\n","    ##\n","    # Turn this print on to see full return data\n","    ##\n","    ##\n","    \"\"\"\n","    e.g.\n","    {\n","      \"id\": \"635cb8d445ujhe5546bb64e5e7\",\n","      \"object\": \"chat.completion\",\n","      \"created\": 170hrjfjf7084,\n","      \"model\": \"open-mistral-7b\",\n","      \"choices\": [\n","        {\n","          \"index\": 0,\n","          \"message\": {\n","            \"role\": \"assistant\",\n","            \"content\": \"Enjoy your cup of tea!\"\n","          },\n","          \"finish_reason\": \"stop\",\n","          \"logprobs\": null\n","        }\n","      ],\n","      \"usage\": {\n","        \"prompt_tokens\": 575,\n","        \"total_tokens\": 629,\n","        \"completion_tokens\": 54\n","      }\n","    }\n","    \"\"\"\n","    # print(json.dumps(response_data, indent=2))\n","    # print(type(response_data))\n","\n","    output = response_data\n","    # print(type(output))\n","    # print(type(output[\"choices\"][0]))\n","\n","    # extract just the 'what they said' part out\n","    assistant_says = output[\"choices\"][0]['message']['content']\n","\n","    # 6. return only reply (no 'history')\n","    return assistant_says\n","\n","\n","def strip_non_alpha(text):\n","    # regex to leave only a-z characters\n","    pattern = re.compile('[^a-z]')\n","    return pattern.sub('', text).lower()\n","\n","\n","def keep_talking(context_history, use_this_model):\n","    \"\"\"\n","    A very minimal chat with memory.\n","\n","    Uses:\n","      query(input_string)\n","      strip_non_alpha(text)\n","    \"\"\"\n","    still_talking = True\n","    dialogue_history = \"\"\n","\n","    while still_talking:\n","\n","        user_input = input(\"Say...\")\n","\n","        exit_phrase_list = [\n","            \"exit\",\n","            \"quit\",\n","            \"quite\",\n","            \"!q\",\n","            \"q\",\n","            \"done\",\n","            \"finish\",\n","            \"end\",\n","            \"bye\",\n","            \"good bye\",\n","        ]\n","\n","        # check if user is exiting convesation\n","        if strip_non_alpha(user_input) in exit_phrase_list:\n","            print(\"\\nAll Done!\")\n","            break\n","\n","        else:\n","            assistant_says, context_history = go_user(user_input, context_history, use_this_model)\n","\n","            print( assistant_says )\n","\n","            # save dialogue so far\n","            dialogue_history = context_history\n","\n","    # when out of loop, return history\n","    return dialogue_history\n","\n","\n","# save history\n","def record_history_save_files(dialogue_history):\n","\n","    date_time = dt.utcnow()\n","    timestamp = date_time.strftime('%Y/%m/%d  %H:%M:%S:%f')\n","    clean_timestamp = date_time.strftime('%Y%m%d%H%M')\n","\n","    # To save the data directly as a JSON file:\n","\n","    # Convert the Python dictionary list to a JSON string\n","    json_data = json.dumps(dialogue_history)\n","\n","    # Open a file for writing in JSON format\n","    with open(f'json_dialog_{clean_timestamp}.json', 'w') as json_file:\n","        # Write the JSON string to the file\n","        json_file.write(json_data)\n","\n","\n","    # To save the data as a file readable as a script:\n","\n","    # Create a new file for writing\n","    with open(f'script_dialog_{clean_timestamp}.txt', 'w') as script_file:\n","\n","        # add timestamp\n","        text = timestamp + \"\\n\\n\"\n","        script_file.write(text)\n","\n","        # Iterate over the dictionary list\n","        for item in dialogue_history:\n","            # Write the role and content of each item to the file, separated by a newline\n","            script_file.write(f\"{item['role']}: {item['content']}\\n\\n\")"]},{"cell_type":"markdown","source":["# Set 'Personality Instruction' and Reset History\n","\n","Tech note: This sets a 'system prompt'"],"metadata":{"id":"t7je4Jg6nU0S"}},{"cell_type":"code","source":["# Reset History\n","context_history = []\n","\n","######################################\n","# Enter your Personality request here\n","######################################\n","\n","# e.g. A Parent\n","personality = \"\"\"You are a helpful chatbot who always answers with the\n","personality or persona and in the style\n","in which this excerpt was written:\n","\n","My dearest Ada,\n","\n","I hope camp is treating you well!  Are you making new friends and exploring all the fun activities? Remember that time we built the world's biggest blanket fort in the living room? You have that same spirit of adventure in you.  Try new things, even if they seem a bit scary at first. Don't be afraid to ask your counselors for help, they're there to make your time awesome.\n","\n","I miss our bedtime stories and silly songs, but I'm so excited for all the stories you'll bring home!  I love you more than all the stars in the campfire sky.\n","\n","Love always,\n","Mom\n","\n","\"\"\"\n","\n","# e.g. A Duck\n","personality = \"\"\"You are a helpful chatbot who always answers in the style\n","of a duck who quacks.\n","\"\"\"\n","\n","# e.g. Generic Warm Person\n","personality = \"\"\"You are a helpful chatbot who always answers in the style\n","of a warm, friendly, outgoing friend, who uses only the language\n","of the user. If someone speaks to you not in English, use only\n","their language to reply. No translation into English, only speak and respond\n","entirely in the language used by User to speak with you.\n","\"\"\"\n","\n","# e.g. Generic Warm Person\n","personality = \"\"\"You are a helpful chatbot who assists\n","with coding questions, and practices good coding style\n","and communication with clear comments and unique\n","names that do not collide.\n","\"\"\"\n","\n","\n","\n","# Save the lower-most 'personality' description from above.\n","context_history.append(add_to_context_history('system', personality))"],"metadata":{"id":"uloouSMrvhRn","executionInfo":{"status":"ok","timestamp":1720459295431,"user_tz":240,"elapsed":194,"user":{"displayName":"On Off","userId":"17227537208113104618"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime as dt\n","\n","# make readable time\n","date_time = dt.utcnow()\n","timestamp = date_time.strftime('%Y/%m/%d  %H:%M:%S:%f')\n","clean_timestamp = date_time.strftime('%Y%m%d%H%M%S%f')\n","\n","instructions = f\"\"\"\n","  Instructions: {timestamp}\n","    - Enter your queries into the iput-box.\n","    - Say 'quit' or 'exit', etc.,  to leave the conversation.\n","\n","\"\"\"\n","print( instructions )\n","\n","print( context_history )\n","\n","# pick model\n","use_this_model = \"open-mistral-7b\"\n","\n","# run chat\n","dialogue_history = keep_talking(context_history, use_this_model)\n","\n","# save history files\n","record_history_save_files(dialogue_history)"],"metadata":{"id":"kcab3MZ-wFIL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720459319309,"user_tz":240,"elapsed":16312,"user":{"displayName":"On Off","userId":"17227537208113104618"}},"outputId":"af08ddfc-4e54-4fff-a8a3-c2975da324c4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","  Instructions: 2024/07/08  17:21:42:941787\n","    - Enter your queries into the iput-box.\n","    - Say 'quit' or 'exit', etc.,  to leave the conversation.\n","\n","\n","[{'role': 'system', 'content': 'You are a helpful chatbot who assists\\nwith coding questions, and practices good coding style\\nand communication with clear comments and unique\\nnames that do not collide.\\n'}]\n","Say...hello\n","Hello! It's nice to meet you. I'm here to help with your coding questions. I strive to use good coding style, communicate clearly with comments, and use unique names for variables and functions to avoid naming collisions. How can I assist you today?\n","Say...exit\n","\n","All Done!\n"]}]},{"cell_type":"code","source":["# simple_ask_mistral_cloud(\"Hellow world\", use_this_model)"],"metadata":{"id":"axk8gi3cagVN"},"execution_count":null,"outputs":[]}]}