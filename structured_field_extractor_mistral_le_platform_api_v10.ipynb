{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya5gXB9W5e9G"
      },
      "source": [
        "# General Structured Data Field Extaction\n",
        "#### Extract structured-fields from unstructured input into a python dictionary\n",
        "\n",
        "\n",
        "## Simple Minimal Data-Structured Task with Mistral API\n",
        "This is a colab-notebook for testing data-extraction using a Mistral cloud model and cloud-api. This colab can run on any online device that runs a web browser (phone, tablet, laptop, desktop, etc.). This is a system for using the online cloud Mistral api, not the mistral models run locally in a local pipeline (e.g. using .gguf llama.cpp).\n",
        "\n",
        "## Steps:\n",
        "1. Configure api-key https://console.mistral.ai/api-keys/\n",
        "2. Select Model https://docs.mistral.ai/getting-started/models/models_overview/\n",
        "3. set \"parameters\" (or leave to default) https://docs.mistral.ai/capabilities/completion/sampling\n",
        "3. Define what fields you want in a 'dictionary'\n",
        "4. Describe those fields (e.g. their data-types, ranges, etc.)\n",
        "5. Configure the extraction-task in the \"Prompt\"\n",
        "6. Run all cells (Use the \"Run all\" button above)\n",
        "7. (Optional) Download the session-log. Use the  folder-icon to the left <- to see the session log files; right click; download.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lybq6TcduDRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note\n",
        "Key parts of the structured-data extraction process include\n",
        "1. forcing a structure, pattern, or delimitor that can be detected in output\n",
        "2. using the pattern or marker to extract the data\n",
        "3. retrying in case of failure\n",
        "4. retrying to check consistency and self-agreement (outlier check, sanity check, etc.)\n",
        "\n",
        "Note: Escape-characters can become a problem with some formats (markdown-json is not always a viable format).\n",
        "\n",
        "These can very significantly between models and sizes of models."
      ],
      "metadata": {
        "id": "n_M0McvXZxnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Raw Input"
      ],
      "metadata": {
        "id": "L5eQpgQK7pDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text_blob = \"\"\"\n",
        "Carl is a Siamese cat. He eats blue milk and wears shoes.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rFO_eyGu7qWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Output Structure (Python Dictionary)\n",
        "e.g.\n",
        "```python\n",
        "target_schema_model_dict = {\n",
        "    \"animal_or_not\": None,\n",
        "    \"favorite_food\": None,\n",
        "    \"species\": None,\n",
        "    \"wearing\": None,\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "6KsIWeNEwQE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this gets added to prompt\n",
        "target_schema_model_dict = {\n",
        "    \"animal_or_not\":  False, # boolean\n",
        "    \"probability_is_animal\": 0.0, # float, confidence level\n",
        "\n",
        "    \"favorite_food\":  \"\", # string\n",
        "    \"probability_of_food\": 0.0, # float, confidence level\n",
        "\n",
        "    \"species\":  \"\", # string\n",
        "    \"wearing\":  \"\", # string\n",
        "\n",
        "    \"noise_level_in_input\": 0, # int, 0-10: 10 is all noise, 0 is clean\n",
        "    \"description_comment\": \"\", # string\n",
        "}"
      ],
      "metadata": {
        "id": "d3r84orewUfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Describe the Data Schema\n",
        "- data types\n",
        "- what the meaning and context of the field is\n",
        "\n",
        "(note: make field names very clear)\n",
        "\n",
        "e.g.\n",
        "```\n",
        "data_schema_doc_blurb = \"\"\"\n",
        "- id or not is boolean\n",
        "- confidence level is 0-1 float, like a probability\n",
        "- comment/description must not be more than 100 characters\n",
        "\"\"\"\n",
        "```\n"
      ],
      "metadata": {
        "id": "EQvlriOsxBdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this gets added to prompt\n",
        "data_schema_doc_blurb = \"\"\"\n",
        "- id or not is boolean\n",
        "- confidence level is 0-1 float, like a probability\n",
        "- comment/description must not be more than 100 characters\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Y1XLbrI1w6sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set 'system prompt'"
      ],
      "metadata": {
        "id": "5OGQkYvq6Jtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_extraction_prompt(\n",
        "    raw_text_input,\n",
        "    data_schema,\n",
        "    description_of_data_schema,\n",
        "):\n",
        "\n",
        "    full_prompt = f\"\"\"\n",
        "\n",
        "    The task is to extract fields of structured data from an unstructured string.\n",
        "\n",
        "    The fields and schema are:\n",
        "    {data_schema}\n",
        "\n",
        "    Descriptions of this including datatypes:\n",
        "    {description_of_data_schema}\n",
        "\n",
        "    The original unstructured text is:\n",
        "    {raw_text_input}\n",
        "\n",
        "    Output must be in correct markdown json format\n",
        "    starting with ```json\n",
        "    ending with ```\n",
        "\n",
        "    Return a structured json object in markdown format,\n",
        "    No other comments or output.\n",
        "    \"\"\"\n",
        "\n",
        "    return full_prompt\n"
      ],
      "metadata": {
        "id": "QVPYWQVU6J9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set extraction_prompt_string\n",
        "extraction_prompt_string = build_extraction_prompt(\n",
        "    raw_text_blob,\n",
        "    target_schema_model_dict,\n",
        "    data_schema_doc_blurb,\n",
        ")"
      ],
      "metadata": {
        "id": "EbCr-GctRpHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_XNy2pxIjg-"
      },
      "source": [
        "# The Core API call:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVci_vEpa6xe"
      },
      "source": [
        "- Select style-prompt to experiment with personality of answer (optional)\n",
        "  - Modify the personality = \"\" text to describe the personality you want.\n",
        "  - Specifying the language of reply can be done in the sytem-prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6fGLCwKndOE"
      },
      "source": [
        "## Notes:\n",
        "\n",
        "# Mistral models and names are updated and changes fairly often, check web for current.\n",
        "Older models will hopefully be available somewhere online if not huggingface.\n",
        "\n",
        "### Note: Colabs are slower\n",
        "Free colabs are amazing for easily sharing and running code in a portable way,\n",
        "but they are slower. Code in production, or run locally, will be faster than a colab.\n",
        "\n",
        "#### From Mistral docs, See:\n",
        "- https://docs.mistral.ai/\n",
        "- https://docs.mistral.ai/getting-started/models/models_overview/\n",
        "- https://docs.mistral.ai/platform/client/\n",
        "\n",
        "```\n",
        "curl --location \"https://api.mistral.ai/v1/chat/completions\" \\\n",
        "     --header 'Content-Type: application/json' \\\n",
        "     --header 'Accept: application/json' \\\n",
        "     --header \"Authorization: Bearer $MISTRAL_API_KEY\" \\\n",
        "     --data '{\n",
        "    \"model\": \"mistral-small-latest\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Who is the most renowned French painter?\"}]\n",
        "  }'\n",
        "```\n",
        "\n",
        "\n",
        "## mistral-small-latest = Mixtral8x7\n",
        "https://mistral.ai/news/mixtral-of-experts/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnSOdXllkJUB"
      },
      "source": [
        "### response = requests.post(endpoint_url, headers=headers, json=request_body)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "THmxT0mUUGTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import json\n",
        "import re\n",
        "import requests"
      ],
      "metadata": {
        "id": "KP19llFDUGe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GtYYzwxHt2s"
      },
      "source": [
        "# login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BghwWvtCHt-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95176db0-f21e-4436-c2c8-f50ab426b2d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nHard Code (not the best idea)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\"\"\"\n",
        ".env: get your environment variables:\n",
        "  Using the Google Secretes (like.env) system\n",
        "  built into colab on the left menu: the 'key' icon.\n",
        "\"\"\"\n",
        "from google.colab import userdata\n",
        "mistral_api_key = userdata.get('mistral_api_key')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Python Dot-env\n",
        "\"\"\"\n",
        "# from dotenv import load_dotenv\n",
        "# import os\n",
        "\n",
        "# load_dotenv()\n",
        "# api_key = os.getenv(\"mistral_api_key\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Hard Code (not the best idea)\n",
        "\"\"\"\n",
        "# mistral_api_key = 'xxx'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjmm0B5nL80T"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIkHiqSUxkMo"
      },
      "source": [
        "Comment out the model you don't want to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLHdP9fqMAIi"
      },
      "outputs": [],
      "source": [
        "# Select Model\n",
        "\"\"\"\n",
        "https://docs.mistral.ai/api/\n",
        "\n",
        "open-mistral-7b\n",
        "\n",
        "open-mixtral-8x22b\n",
        "open-mixtral-8x22b-2404\n",
        "\n",
        "codestral-latest\n",
        "codestral-2405\n",
        "\n",
        "\n",
        "open-mistral-7b\n",
        "(aka mistral-tiny-2312)\n",
        "renamed from mistral-tiny\n",
        "The endpoint mistral-tiny will be deprecated\n",
        "\n",
        "\n",
        "Feb. 26, 2024\n",
        "\n",
        "API endpoints: We renamed 3 API endpoints and added 2 model endpoints.\n",
        "\n",
        "open-mistral-7b (aka mistral-tiny-2312): renamed from mistral-tiny. The endpoint mistral-tiny will be deprecated in three months.\n",
        "open-mixtral-8x7B (aka mistral-small-2312): renamed from mistral-small. The endpoint mistral-small will be deprecated in three months.\n",
        "mistral-small-latest (aka mistral-small-2402): new model.\n",
        "mistral-medium-latest (aka mistral-medium-2312): old model. The previous mistral-medium has been dated and tagged as mistral-medium-2312. The endpoint mistral-medium will be deprecated in three months.\n",
        "mistral-large-latest (aka mistral-large-2402): our new flagship model with leading performance.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "##################\n",
        "# Open Mistral 7b\n",
        "##################\n",
        "# previously \"tiny\"\n",
        "use_this_model = \"open-mistral-7b\"\n",
        "\n",
        "\n",
        "###################\n",
        "# Open Mixtral 8x7\n",
        "###################\n",
        "# previously \"small\"\n",
        "use_this_model = \"open-mixtral-8x7B\"\n",
        "\n",
        "\n",
        "######################\n",
        "# open mixtral 8x22b\n",
        "######################\n",
        "# ...was 'medium'?\n",
        "use_this_model = \"open-mixtral-8x22b\"\n",
        "\n",
        "\n",
        "#######################\n",
        "# Small, Medium, Large  (no 'tiny')\n",
        "#######################\n",
        "use_this_model = \"mistral-small-latest\"\n",
        "use_this_model = \"mistral-medium-latest\"\n",
        "use_this_model = \"mistral-large-latest\"\n",
        "\n",
        "##############\n",
        "# Codestral\n",
        "##############\n",
        "use_this_model = \"codestral-latest\"\n",
        "\n",
        "\n",
        "use_this_model = \"open-mistral-7b\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paremeters\n",
        "https://docs.mistral.ai/capabilities/completion/sampling"
      ],
      "metadata": {
        "id": "_SWx_I8ZuavG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEMPERATURE = 0.8 # higher number is more ~creative/variable answer\n",
        "\n",
        "\"\"\"\n",
        "Higher number accepts larger range of options,\n",
        "or options with lower probabilities.\n",
        "0.5 looks only at options with the top 50% likelihoods.\n",
        "\"\"\"\n",
        "TOP_P = 0.5  # top % (as fraction) considered for tokens,\n",
        "\n",
        "\"\"\"\n",
        "Range: [-2, 2]\n",
        "Default: 0\n",
        "\"\"\"\n",
        "PRESENCE_PENALTY = 0\n",
        "\n",
        "FREQUENCY_PENALTY = 0\n"
      ],
      "metadata": {
        "id": "g5ZizYzxua2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2nfxcae5plc"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Npzlz9WGm549"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "# import json\n",
        "# import os\n",
        "# import re\n",
        "# from google.colab import userdata\n",
        "\n",
        "\"\"\"\n",
        "# mistral_api_key = userdata.get('mistral_api_key')\n",
        "\n",
        "# Define the endpoint URL\n",
        "endpoint_url = \"https://api.mistral.ai/v1/chat/completions\"\n",
        "\n",
        "# Set the headers\n",
        "headers = {\n",
        "  \"Content-Type\": \"application/json\",\n",
        "  \"Accept\": \"application/json\",\n",
        "  \"Authorization\": f\"Bearer {mistral_api_key}\"\n",
        "}\n",
        "\n",
        "# mode: [{\"role\": \"user\", \"content\": \"say yes\"}]\n",
        "\n",
        "    # Define the request body\n",
        "    request_body = {\n",
        "      \"model\": \"mistral-small-latest\",\n",
        "      \"messages\": [{\"role\": \"user\", \"content\": user_input}]\n",
        "    }\n",
        "\n",
        "    # Send the request\n",
        "    response = requests.post(endpoint_url, headers=headers, json=request_body)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def add_to_context_history(role, comment):\n",
        "\n",
        "    if role == 'user':\n",
        "        segment = {\"role\": \"user\", \"content\": comment}\n",
        "\n",
        "    elif role == 'assistant':\n",
        "        segment = {\"role\": \"assistant\", \"content\": comment}\n",
        "\n",
        "    elif role == 'system':\n",
        "        segment = {\"role\": \"system\", \"content\": comment}\n",
        "\n",
        "    else:\n",
        "        print(\"add_to_context_history(role, comment)\")\n",
        "        print(role, comment)\n",
        "        print('error')\n",
        "\n",
        "    return segment\n",
        "\n",
        "\n",
        "def ask_mistral_api(context_history, use_this_model):\n",
        "\n",
        "\n",
        "    # Define the endpoint URL\n",
        "    endpoint_url = \"https://api.mistral.ai/v1/chat/completions\"\n",
        "\n",
        "    # Set the headers\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Accept\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {mistral_api_key}\"\n",
        "    }\n",
        "\n",
        "    # Define the request body\n",
        "    request_body = {\n",
        "        \"model\": use_this_model,\n",
        "        \"messages\": context_history,\n",
        "\n",
        "        \"temperature\": TEMPERATURE,\n",
        "        \"top_p\": TOP_P,\n",
        "        \"presence_penalty\": PRESENCE_PENALTY,\n",
        "        \"frequency_penalty\": FREQUENCY_PENALTY,\n",
        "    }\n",
        "\n",
        "    #################\n",
        "    #################\n",
        "    # Hit the ai api\n",
        "    #################\n",
        "    #################\n",
        "    # Send the request\n",
        "    response = requests.post(endpoint_url, headers=headers, json=request_body)\n",
        "\n",
        "    # Check the response status code\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Error: {response.status_code} {response.text}\")\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "def simple_ask_mistral_cloud(input_string, use_this_model):\n",
        "    \"\"\"\n",
        "    you have: a string\n",
        "    you need: a response\n",
        "\n",
        "    1. make minimal history contexxt\n",
        "    2. make a generic system instruction, for show\n",
        "    3. make system-user context: string input\n",
        "    4. ask mistral for that model\n",
        "    5. extract just the response string\n",
        "    6. return only reply (no 'history')\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. make minimal history contexxt\n",
        "    context_history = []\n",
        "\n",
        "    # 2. make a generic system instruction\n",
        "    generic_system_instruction = \"You are helpful and answer accurately.\"\n",
        "    context_history.append( add_to_context_history(\"system\", generic_system_instruction) )\n",
        "\n",
        "    # 3. make system-user context: string input\n",
        "    context_history.append( add_to_context_history(\"user\", input_string) )\n",
        "\n",
        "    # 4. ask mistral for that model\n",
        "    response = ask_mistral_api(context_history, use_this_model)\n",
        "\n",
        "\n",
        "    # Get the response data\n",
        "    response_data = response.json()\n",
        "\n",
        "\n",
        "    # 5. extract just the response string\n",
        "\n",
        "    ##\n",
        "    ##\n",
        "    # Turn this print on to see full return data\n",
        "    ##\n",
        "    ##\n",
        "    \"\"\"\n",
        "    e.g.\n",
        "    {\n",
        "      \"id\": \"635cb8d445ujhe5546bb64e5e7\",\n",
        "      \"object\": \"chat.completion\",\n",
        "      \"created\": 170hrjfjf7084,\n",
        "      \"model\": \"open-mistral-7b\",\n",
        "      \"choices\": [\n",
        "        {\n",
        "          \"index\": 0,\n",
        "          \"message\": {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"Enjoy your cup of tea!\"\n",
        "          },\n",
        "          \"finish_reason\": \"stop\",\n",
        "          \"logprobs\": null\n",
        "        }\n",
        "      ],\n",
        "      \"usage\": {\n",
        "        \"prompt_tokens\": 575,\n",
        "        \"total_tokens\": 629,\n",
        "        \"completion_tokens\": 54\n",
        "      }\n",
        "    }\n",
        "    \"\"\"\n",
        "    # print(json.dumps(response_data, indent=2))\n",
        "    # print(type(response_data))\n",
        "\n",
        "    output = response_data\n",
        "    # print(type(output))\n",
        "    # print(type(output[\"choices\"][0]))\n",
        "\n",
        "    # extract just the 'what they said' part out\n",
        "    assistant_says = output[\"choices\"][0]['message']['content']\n",
        "\n",
        "    # 6. return only reply (no 'history')\n",
        "    return assistant_says\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM1T1-KU2COb"
      },
      "source": [
        "# Extraction Code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Helper Function\n",
        "def extract_json_as_pydict_from_unstructured_text(dict_str, model_dict):\n",
        "    \"\"\"\n",
        "    This function CAN fail and should fail\n",
        "    if the AI needs to retry at a task.\n",
        "    Do not stop server when this this triggers an exception.\n",
        "\n",
        "    edge case: before there is a populated output_log\n",
        "\n",
        "    if passing, this function will return a valid json object\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Extracts JSON string enclosed between ```json and ``` markers.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The input text containing the JSON block.\n",
        "\n",
        "    Returns:\n",
        "    - str: The extracted JSON string, or an empty string if no JSON block is found.\n",
        "    \"\"\"\n",
        "    print(f\"\\n\\n Starting check_function_description_keys, dict_str -> {dict_str}\")\n",
        "\n",
        "    ########################\n",
        "    # Check Json Formatting\n",
        "    ########################\n",
        "    try:\n",
        "        pattern = r'```json\\n([\\s\\S]*?)\\n```'\n",
        "        match = re.search(pattern, dict_str)\n",
        "        dict_str =  match.group(1) if match else ''\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTRY AGAIN: check_function_description_keys() extraction from markdown failed: {e}\")\n",
        "        print(f\"Failed dict_str -> {dict_str}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"\\n extracted from markdown ->{dict_str}\")\n",
        "\n",
        "    # clean\n",
        "    try:\n",
        "        # try safety cleaning\n",
        "        dict_str = dict_str.replace(\"True\", \"true\")\n",
        "        dict_str = dict_str.replace(\"False\", \"false\")\n",
        "        dict_str = dict_str.replace(\"None\", \"null\")\n",
        "\n",
        "        # # This conflicted with free language in description section...\n",
        "        # dict_str = dict_str.replace(\"'\", '\"')\n",
        "\n",
        "        # remove trailing delimiter comma\n",
        "        print(f\"{dict_str[:-6]}\")\n",
        "        dict_str = dict_str.replace('\",\\n}', '\"\\n}')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTRY AGAIN:try safety cleaning: {e}\")\n",
        "        print(f\"Failed dict_str -> {dict_str}\")\n",
        "        return False\n",
        "\n",
        "    # load\n",
        "    try:\n",
        "        # try converting\n",
        "        dict_str = json.loads(dict_str)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTRY AGAIN: trying json.loads(dict_str) Dictionary load failed: {e}\")\n",
        "        print(f\"Failed dict_str -> {dict_str}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "    # check if keys are the same\n",
        "    try:\n",
        "        result = dict_str.keys() == model_dict.keys()\n",
        "        if result is False:\n",
        "            print(f\"Failed: keys are not the same.\")\n",
        "            print(f\"Failed dict_str -> {dict_str}\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTRY AGAIN: Failed with Exception: keys are not the same: {e}\")\n",
        "        print(f\"Failed dict_str -> {dict_str}\")\n",
        "        return False\n",
        "\n",
        "    # if ok...\n",
        "    return dict_str\n",
        "\n"
      ],
      "metadata": {
        "id": "EhVj_VY8zSMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "import re\n",
        "import traceback\n",
        "# from datetime import datetime\n",
        "\n",
        "\n",
        "def initialize_session_log(\n",
        "    raw_text_blob: str,\n",
        "    target_schema_model_dict: dict,\n",
        "    use_this_model: str,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Creates a new timestamped session log file and writes\n",
        "    the opening configuration block to it.\n",
        "\n",
        "    Everything printed during the session will also be\n",
        "    appended to this file via print_and_log().\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    raw_text_blob : str\n",
        "        The raw input text to be processed.\n",
        "    target_schema_model_dict : dict\n",
        "        The schema dict defining expected output fields.\n",
        "    use_this_model : str\n",
        "        The model name string used for this session.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        The filepath of the created session log file.\n",
        "    \"\"\"\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n",
        "    session_log_filepath = f\"session_log_{timestamp}.txt\"\n",
        "\n",
        "    opening_block = (\n",
        "        f\"\\n{'='*60}\\n\"\n",
        "        f\"SESSION LOG\\n\"\n",
        "        f\"{'='*60}\\n\"\n",
        "        f\"Timestamp      : {timestamp}\\n\"\n",
        "        f\"Model          : {use_this_model}\\n\"\n",
        "        f\"Schema         :\\n{json.dumps(target_schema_model_dict, indent=2)}\\n\"\n",
        "        f\"Raw Input Blob :\\n{raw_text_blob}\\n\"\n",
        "        f\"{'='*60}\\n\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        with open(session_log_filepath, 'w') as log_file:\n",
        "            log_file.write(opening_block)\n",
        "        print(opening_block)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Could not create session log file: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "    return session_log_filepath\n",
        "\n",
        "\n",
        "def print_and_log(\n",
        "    message: str,\n",
        "    session_log_filepath: str,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Prints a message to stdout AND appends it to the session log file.\n",
        "\n",
        "    This is the single point of output throughout the pipeline.\n",
        "    Nothing should be printed directly — always use this function\n",
        "    so that stdout and the log file stay in sync.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    message : str\n",
        "        The string to print and append to the log.\n",
        "    session_log_filepath : str\n",
        "        Path to the active session log file.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    print(message)\n",
        "\n",
        "    try:\n",
        "        with open(session_log_filepath, 'a') as log_file:\n",
        "            log_file.write(message + \"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Print warning to stdout only — do not recurse into print_and_log\n",
        "        print(f\"WARNING: Could not append to session log file: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "\n",
        "def save_extraction_results(\n",
        "    raw_text_blob: str,\n",
        "    target_schema_model_dict: dict,\n",
        "    use_this_model: str,\n",
        "    result_dict: dict | None,\n",
        "    attempts_made: int,\n",
        "    session_log_filepath: str,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Saves the final extraction results to a timestamped JSON file.\n",
        "\n",
        "    Called on success (with the validated dict) and on total failure\n",
        "    (with result_dict=None). Captures all inputs, settings, and output\n",
        "    together in one file for reproducibility and inspection.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    raw_text_blob : str\n",
        "        The original raw input text.\n",
        "    target_schema_model_dict : dict\n",
        "        The schema dict.\n",
        "    use_this_model : str\n",
        "        Model name string.\n",
        "    result_dict : dict or None\n",
        "        The validated extracted dict on success, or None on failure.\n",
        "    attempts_made : int\n",
        "        How many attempts were made before this result.\n",
        "    session_log_filepath : str\n",
        "        Path to the session log file (recorded here for cross-reference).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n",
        "    results_filepath = f\"extraction_results_{timestamp}.json\"\n",
        "\n",
        "    results_payload = {\n",
        "        \"timestamp\": timestamp,\n",
        "        \"model_used\": use_this_model,\n",
        "        \"attempts_made\": attempts_made,\n",
        "        \"raw_text_blob\": raw_text_blob,\n",
        "        \"target_schema_model_dict\": target_schema_model_dict,\n",
        "        \"extraction_result\": result_dict,\n",
        "        \"session_log_filepath\": session_log_filepath,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with open(results_filepath, 'w') as results_file:\n",
        "            json.dump(results_payload, results_file, indent=2)\n",
        "\n",
        "        print_and_log(\n",
        "            f\"\\nResults saved to: {results_filepath}\",\n",
        "            session_log_filepath,\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print_and_log(\n",
        "            f\"WARNING: Could not save results file: {e}\\n{traceback.format_exc()}\",\n",
        "            session_log_filepath,\n",
        "        )\n",
        "\n",
        "\n",
        "def run_extraction_with_retry(\n",
        "    raw_text_blob: str,\n",
        "    target_schema_model_dict: dict,\n",
        "    extraction_prompt_string: str,\n",
        "    use_this_model: str,\n",
        "    max_retries: int = 3,\n",
        ") -> dict | None:\n",
        "    \"\"\"\n",
        "    Main extraction pipeline loop.\n",
        "\n",
        "    Takes a fully pre-built prompt string (already assembled by\n",
        "    build_extraction_prompt() before this function is called),\n",
        "    sends it to the Mistral API, and attempts to extract a validated\n",
        "    Python dict matching the target schema.\n",
        "\n",
        "    Retries up to max_retries times on JSON extraction failure.\n",
        "\n",
        "    All output — raw API responses, extraction attempts, successes,\n",
        "    failures — is printed to stdout AND appended to a timestamped\n",
        "    session log file via print_and_log().\n",
        "\n",
        "    Final results (success or failure) are saved to a separate\n",
        "    timestamped JSON file via save_extraction_results().\n",
        "\n",
        "    Context history is constructed ONCE before the loop.\n",
        "    The prompt does not change between retries.\n",
        "\n",
        "    API-level exceptions (non-200 status from ask_mistral_api())\n",
        "    are NOT caught here — they propagate up and stop execution.\n",
        "    JSON extraction failures are caught and retried.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    raw_text_blob : str\n",
        "        The original raw input text. Used for saving results.\n",
        "        The prompt string is already built before this is called.\n",
        "    target_schema_model_dict : dict\n",
        "        The schema dict defining expected output fields and structure.\n",
        "        Passed to extract_json_as_pydict_from_unstructured_text()\n",
        "        for key validation.\n",
        "    extraction_prompt_string : str\n",
        "        The fully assembled prompt string from build_extraction_prompt().\n",
        "    use_this_model : str\n",
        "        The Mistral model name string.\n",
        "    max_retries : int\n",
        "        Maximum number of API call + extraction attempts. Default 3.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Validated Python dict matching the schema on success.\n",
        "    None\n",
        "        If all retries are exhausted without a valid extraction.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Initialize session log (opened once, appended throughout) ---\n",
        "    session_log_filepath = initialize_session_log(\n",
        "        raw_text_blob,\n",
        "        target_schema_model_dict,\n",
        "        use_this_model,\n",
        "    )\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    # Construct context_history ONCE before the loop.\n",
        "    # Same history is sent on every retry attempt.\n",
        "    # ----------------------------------------------------------------\n",
        "    context_history = []\n",
        "\n",
        "    generic_system_instruction = (\n",
        "        \"You are a precise data extraction assistant. \"\n",
        "        \"Follow the instructions exactly. \"\n",
        "        \"Return only valid JSON in markdown format. \"\n",
        "        \"No other text or commentary.\"\n",
        "    )\n",
        "\n",
        "    # system message\n",
        "    context_history.append(\n",
        "        add_to_context_history(\"system\", generic_system_instruction)\n",
        "    )\n",
        "\n",
        "    # user message: the full pre-built prompt\n",
        "    context_history.append(\n",
        "        add_to_context_history(\"user\", extraction_prompt_string)\n",
        "    )\n",
        "\n",
        "    print_and_log(\n",
        "        (\n",
        "            f\"\\nContext history constructed.\"\n",
        "            f\"\\nStarting extraction loop.\"\n",
        "            f\"\\nMax retries: {max_retries}\\n\"\n",
        "        ),\n",
        "        session_log_filepath,\n",
        "    )\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    # Retry loop\n",
        "    # ----------------------------------------------------------------\n",
        "    for attempt_number in range(1, max_retries + 1):\n",
        "\n",
        "        print_and_log(\n",
        "            f\"\\n{'='*50}\\nATTEMPT {attempt_number} of {max_retries}\\n{'='*50}\",\n",
        "            session_log_filepath,\n",
        "        )\n",
        "\n",
        "        # --- Call the API ---\n",
        "        # Note: ask_mistral_api() raises on non-200 — let it propagate.\n",
        "        response = ask_mistral_api(context_history, use_this_model)\n",
        "\n",
        "        # --- Extract raw reply string from response object ---\n",
        "        try:\n",
        "            response_data = response.json()\n",
        "            raw_reply_string = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "        except Exception as e:\n",
        "            print_and_log(\n",
        "                (\n",
        "                    f\"\\nATTEMPT {attempt_number}: Could not extract reply string\"\n",
        "                    f\" from response object.\\n{e}\\n{traceback.format_exc()}\"\n",
        "                ),\n",
        "                session_log_filepath,\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        # --- Print and log the complete raw API reply ---\n",
        "        print_and_log(\n",
        "            f\"\\nRAW API RESPONSE (attempt {attempt_number}):\\n{raw_reply_string}\",\n",
        "            session_log_filepath,\n",
        "        )\n",
        "\n",
        "        # --- Attempt JSON extraction and schema key validation ---\n",
        "        print_and_log(\n",
        "            f\"\\nAttempting JSON extraction and validation (attempt {attempt_number})...\",\n",
        "            session_log_filepath,\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            validated_dict = extract_json_as_pydict_from_unstructured_text(\n",
        "                raw_reply_string,\n",
        "                target_schema_model_dict,\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print_and_log(\n",
        "                (\n",
        "                    f\"\\nATTEMPT {attempt_number}: extract_json_as_pydict raised\"\n",
        "                    f\" an exception.\\n{e}\\n{traceback.format_exc()}\"\n",
        "                ),\n",
        "                session_log_filepath,\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        # --- Check result: False means malformed or wrong keys ---\n",
        "        if validated_dict is False:\n",
        "            print_and_log(\n",
        "                (\n",
        "                    f\"\\nATTEMPT {attempt_number} FAILED:\"\n",
        "                    f\" extraction returned False (malformed JSON or wrong keys).\"\n",
        "                    f\" Retrying...\\n\"\n",
        "                ),\n",
        "                session_log_filepath,\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        # ------------------------------------------------------------\n",
        "        # SUCCESS\n",
        "        # ------------------------------------------------------------\n",
        "        print_and_log(\n",
        "            (\n",
        "                f\"\\nATTEMPT {attempt_number} SUCCEEDED.\\n\"\n",
        "                f\"Extracted dict:\\n{json.dumps(validated_dict, indent=2)}\"\n",
        "            ),\n",
        "            session_log_filepath,\n",
        "        )\n",
        "\n",
        "        save_extraction_results(\n",
        "            raw_text_blob=raw_text_blob,\n",
        "            target_schema_model_dict=target_schema_model_dict,\n",
        "            use_this_model=use_this_model,\n",
        "            result_dict=validated_dict,\n",
        "            attempts_made=attempt_number,\n",
        "            session_log_filepath=session_log_filepath,\n",
        "        )\n",
        "\n",
        "        return validated_dict\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    # Loop exhausted — all attempts failed\n",
        "    # ----------------------------------------------------------------\n",
        "    print_and_log(\n",
        "        (\n",
        "            f\"\\nWARNING: All {max_retries} attempts exhausted.\"\n",
        "            f\" Extraction failed.\"\n",
        "            f\" Returning None.\\n\"\n",
        "        ),\n",
        "        session_log_filepath,\n",
        "    )\n",
        "\n",
        "    save_extraction_results(\n",
        "        raw_text_blob=raw_text_blob,\n",
        "        target_schema_model_dict=target_schema_model_dict,\n",
        "        use_this_model=use_this_model,\n",
        "        result_dict=None,\n",
        "        attempts_made=max_retries,\n",
        "        session_log_filepath=session_log_filepath,\n",
        "    )\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "4CpkpAEMLWg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "SwzQss7xLx9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Input Fields\n",
        "\n",
        "def run_extraction_with_retry(\n",
        "    raw_text_blob: str,\n",
        "    target_schema_model_dict: dict,\n",
        "    extraction_prompt_string: str,\n",
        "    use_this_model: str,\n",
        "    max_retries: int = 3,\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "run_extraction_result = run_extraction_with_retry(\n",
        "    raw_text_blob,\n",
        "    target_schema_model_dict,\n",
        "    extraction_prompt_string,\n",
        "    use_this_model,\n",
        "    max_retries=3,\n",
        ")\n",
        "\n",
        "print(f\"run_extraction_result -> {run_extraction_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZKTIjJbLy-M",
        "outputId": "276aeef5-1000-4d7f-a35d-b1236cb9df3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SESSION LOG\n",
            "============================================================\n",
            "Timestamp      : 20260220_204636_959162\n",
            "Model          : open-mistral-7b\n",
            "Schema         :\n",
            "{\n",
            "  \"animal_or_not\": false,\n",
            "  \"probability_is_animal\": 0.0,\n",
            "  \"favorite_food\": \"\",\n",
            "  \"probability_of_food\": 0.0,\n",
            "  \"species\": \"\",\n",
            "  \"wearing\": \"\",\n",
            "  \"noise_level_in_input\": 0,\n",
            "  \"description_comment\": \"\"\n",
            "}\n",
            "Raw Input Blob :\n",
            "\n",
            "Carl is a siamese cat, who eats blue milk and wears shoes\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            "Context history constructed.\n",
            "Starting extraction loop.\n",
            "Max retries: 3\n",
            "\n",
            "\n",
            "==================================================\n",
            "ATTEMPT 1 of 3\n",
            "==================================================\n",
            "\n",
            "RAW API RESPONSE (attempt 1):\n",
            "```json\n",
            "{\n",
            "  \"animal_or_not\": true,\n",
            "  \"probability_is_animal\": 1.0,\n",
            "  \"favorite_food\": \"blue milk\",\n",
            "  \"probability_of_food\": 1.0,\n",
            "  \"species\": \"Siamese cat\",\n",
            "  \"wearing\": \"shoes\",\n",
            "  \"noise_level_in_input\": 0,\n",
            "  \"description_comment\": \"\"\n",
            "}\n",
            "```\n",
            "\n",
            "Attempting JSON extraction and validation (attempt 1)...\n",
            "\n",
            "\n",
            " Starting check_function_description_keys, dict_str -> ```json\n",
            "{\n",
            "  \"animal_or_not\": true,\n",
            "  \"probability_is_animal\": 1.0,\n",
            "  \"favorite_food\": \"blue milk\",\n",
            "  \"probability_of_food\": 1.0,\n",
            "  \"species\": \"Siamese cat\",\n",
            "  \"wearing\": \"shoes\",\n",
            "  \"noise_level_in_input\": 0,\n",
            "  \"description_comment\": \"\"\n",
            "}\n",
            "```\n",
            "\n",
            " extracted from markdown ->{\n",
            "  \"animal_or_not\": true,\n",
            "  \"probability_is_animal\": 1.0,\n",
            "  \"favorite_food\": \"blue milk\",\n",
            "  \"probability_of_food\": 1.0,\n",
            "  \"species\": \"Siamese cat\",\n",
            "  \"wearing\": \"shoes\",\n",
            "  \"noise_level_in_input\": 0,\n",
            "  \"description_comment\": \"\"\n",
            "}\n",
            "{\n",
            "  \"animal_or_not\": true,\n",
            "  \"probability_is_animal\": 1.0,\n",
            "  \"favorite_food\": \"blue milk\",\n",
            "  \"probability_of_food\": 1.0,\n",
            "  \"species\": \"Siamese cat\",\n",
            "  \"wearing\": \"shoes\",\n",
            "  \"noise_level_in_input\": 0,\n",
            "  \"description_comment\"\n",
            "\n",
            "ATTEMPT 1 SUCCEEDED.\n",
            "Extracted dict:\n",
            "{\n",
            "  \"animal_or_not\": true,\n",
            "  \"probability_is_animal\": 1.0,\n",
            "  \"favorite_food\": \"blue milk\",\n",
            "  \"probability_of_food\": 1.0,\n",
            "  \"species\": \"Siamese cat\",\n",
            "  \"wearing\": \"shoes\",\n",
            "  \"noise_level_in_input\": 0,\n",
            "  \"description_comment\": \"\"\n",
            "}\n",
            "\n",
            "Results saved to: extraction_results_20260220_204637_754442.json\n",
            "run_extraction_result -> {'animal_or_not': True, 'probability_is_animal': 1.0, 'favorite_food': 'blue milk', 'probability_of_food': 1.0, 'species': 'Siamese cat', 'wearing': 'shoes', 'noise_level_in_input': 0, 'description_comment': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test API\n"
      ],
      "metadata": {
        "id": "3WDcH5X5LYiV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axk8gi3cagVN"
      },
      "outputs": [],
      "source": [
        "## Optional Test of api system / connection\n",
        "# simple_ask_mistral_cloud(\"Hellow world\", use_this_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-GtYYzwxHt2s",
        "j2nfxcae5plc",
        "qM1T1-KU2COb"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}